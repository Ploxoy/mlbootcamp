{
    "collab_server" : "",
    "contents" : "# Возвращает число, которое больше всего раз встречается в векторе x\ncountMax = function(x) {\n  as.integer(names(sort(table(x),decreasing=T))[1])\n}\n\ncalculateError = function(XL, classifier) {\n  errors = 0\n  n = nrow(XL)\n  m = ncol(XL)\n  for(i in 1:n) {\n    x = XL[i, -m]\n    y = XL[i, m]\n    if (classifier(x) != y) {\n      errors = errors + 1\n    }\n  }\n  return( errors / n )\n}\n\ndonskoiInform01Criteria = function(XL, pred) {\n  res = 0\n  n = nrow(XL)\n  m = ncol(XL)\n  c01 = 0\n  c10 = 0\n  c00 = 0\n  c11 = 0\n  for (i in 1:n) {\n    v = pred(XL[i, ])\n    if (is.na(v))\n      next\n      \n    if (XL[i, m] == 0 && v)\n      c01 = c01 + 1\n    else if (XL[i, m] == 1 && !v)\n      c10 = c10 + 1\n    else if (XL[i, m] == 0 && !v)\n      c00 = c00 + 1\n    else\n      c11 = c11 + 1\n  }\n  return( 2 * (c01 * c10 + c00 * c11) )\n}\n\ndonskoiInformCriteria = function(XL, pred) {\n  res = 0\n  for(i in 1:nrow(XL)) {\n    for(j in 1:nrow(XL)) {\n      if (XL[i,ncol(XL)] != XL[j,ncol(XL)] && pred(XL[i,]) != pred(XL[j,])) {\n        res = res + 1\n      }\n    }\n  }\n  return( res )\n}\n\nID3.treeClassifier = function(XL, predicates, maxDepth = 1000) {\n  m = ncol(XL)\n  buildTreeRec = function(XL, predicates, depth) {\n    p = length(predicates)\n    L = nrow(XL)\n    # если закончились предикаты, или достингута максимальная глубина\n    # выбрать класс, который больше раз встречается\n    if (p == 0 || depth >= maxDepth) {\n      return( list(countMax(XL[,m])) )\n    }\n    \n    # если все одного класса, то выбрать этот класс\n    if (sum(XL[,m] == XL[1, m]) == L) {\n      return( list(XL[1, m]) )\n    }\n    \n    classes = unique(XL[,m])\n    \n    # ищем предикат с максимальной информативностью\n    predicateIdx = -1\n    maxInform = -1e10\n    for(i in 1:p) {\n      pred = predicates[[i]]\n      inf = donskoiInform01Criteria(XL, pred)#!!!!!!!!!!\n      if (inf >= maxInform) {\n        maxInform = inf\n        predicateIdx = i\n      }\n    }\n    pred = predicates[[predicateIdx]] # вот он\n    qwe = c()\n    for(i in 1:L) {\n      qwe = c(qwe, pred(XL[i,]))\n    }\n    failback = countMax(XL[, m])\n    \n    U0 = XL[which(!qwe),,drop=F]\n    U1 = XL[which(qwe),,drop=F]\n    if (nrow(U0) == 0 || nrow(U1) == 0) {\n      return( list(countMax(XL[,m])) )\n    }\n    L = buildTreeRec(U0, predicates[-predicateIdx], depth + 1)\n    R = buildTreeRec(U1, predicates[-predicateIdx], depth + 1)\n    return( list(L, R, pred, failback) )\n  }\n  tree = buildTreeRec(XL, predicates, 1)\n  \n  classify = function(tree, x) {\n    if (length(tree) == 1)\n      return( tree[[1]] )\n    \n    pred = tree[[3]]\n    test = pred(x)\n    if (is.na(test)) {\n      return( tree[[4]] ) # failback\n    }\n    if (test)\n      return( classify(tree[[2]], x) )\n    return( classify(tree[[1]], x) )\n  }\n  return( function(x) { classify(tree, x) } )\n}\n\nID3.classifier = function(aggregator, XL, predicates, treesCount=100, treesDepth=3, partsFactor = 0.3, predicatesFactor = 1) {\n  simpleAlgos = c()\n  for(i in 1:treesCount) {\n    subXL = XL[sample(1:nrow(XL), nrow(XL)*partsFactor), ]\n    subPreds = predicates[sample(1:length(predicates), length(predicates) * predicatesFactor)]\n    simpleAlgos = c(simpleAlgos, ID3.treeClassifier(subXL, subPreds, treesDepth))\n    \n    if (T) {\n      print(paste0(treesCount - i, ' simple algos remains'))\n    }\n  }\n  \n  return( aggregator(XL, simpleAlgos) )\n}\n\nrandomForestTreeAggregator = function (XL, baseAlgos) {\n  return( function(x) {\n    results = c()\n    for (algo in baseAlgos)\n      results = c(results, algo(x))\n    return( countMax(results) )\n  } )\n}\n\nadaBoostAggregator = function(XL, baseAlgos) {\n  n = nrow(XL)\n  m = ncol(XL)\n  weight = rep(1/n, times=n)\n  alpha = c()\n  algo = c()\n  \n  algoResults = matrix(NA, length(baseAlgos), n)\n  for (i in 1:length(baseAlgos))\n    for (j in 1:n)\n      algoResults[i, j] = baseAlgos[[i]](XL[j, -m])\n  \n  algoUsages = rep(0, length(baseAlgos))\n  \n  while(sum(algoUsages) < length(baseAlgos)) {\n    # Находим классификатор который минимизирует взвешенную ошибку классификации\n    minError = 1e10\n    selIdx = NA\n    for(j in 1:length(baseAlgos)) {\n      if (algoUsages[j] > 0)\n        next\n      \n      error = 0\n      for(i in 1:n) {\n        if (algoResults[j, i] != XL[i, m]) {\n          error = error + weight[i]\n        }\n      }\n      if (error < minError) {\n        minError = error\n        selIdx = j\n      }\n    }\n    if (abs(minError - 1) < 1e-5) {\n      break\n    }\n    \n    a = log((1-minError)/minError)/2\n    \n    for(i in 1:n)\n      weight[i] = weight[i] * exp(-a * ifelse(XL[i, m] == algoResults[selIdx, i], 1, -1))\n    weight = weight / sum(weight)\n    \n    algo = c(algo, baseAlgos[[selIdx]])\n    alpha = c(alpha, a)\n\n    algoUsages[selIdx] = algoUsages[selIdx] + 1\n    \n    if (T) {\n      print(paste0(length(baseAlgos) - sum(algoUsages), ' adaBoost algos remains'))\n    }\n  }\n  \n  classes = unique(XL[, m])\n  \n  return( function(x) {\n    cnt = rep(0, times=length(classes))\n    for(i in 1:length(algo)) {\n      b = algo[[i]]\n      cl = b(x)\n      for(j in 1:length(classes)) {\n        if (classes[j] == cl) {\n          cnt[j] = cnt[j] + alpha[i]\n        }\n      }\n    }\n    return( classes[which.max(cnt)] )\n  } )\n}",
    "created" : 1486812837855.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2954390764",
    "id" : "169FE04",
    "lastKnownWriteTime" : 1486845086,
    "last_content_update" : 1486845086622,
    "path" : "G:/Projects/mlbootcamp/sandbox_train/algos.R",
    "project_path" : "algos.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}